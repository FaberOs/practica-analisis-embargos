{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (1.51.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: plotly in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (6.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (6.33.1)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (6.2.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (12.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from plotly) (2.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\andre\\downloads\\embargos\\practica-analisis-embargos\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas streamlit numpy plotly scikit-learn xgboost matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKVLpZ7tNqHt",
    "outputId": "301564dc-845d-49a2-d2ce-20e5cb3c49c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo: consulta detalle embargos-2023-01.csv\n",
      "Leyendo archivo: consulta detalle embargos-2023-02.csv\n",
      "Leyendo archivo: consulta detalle embargos-2024-01.csv\n",
      "Leyendo archivo: consulta detalle embargos-2024-02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_21600\\3618614130.py:83: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_muestreado = df.groupby('mes', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo final consolidado y muestreado: embargos_consolidado_mensual.csv\n",
      "Filas originales: 2227458, tras muestreo: 155924\n",
      "\n",
      "Ejemplo de datos:\n",
      "            id       ciudad  \\\n",
      "0  23052600002        HONDA   \n",
      "1  23053101021  BUCARAMANGA   \n",
      "2  23052700466   BOGOTÁ D.C   \n",
      "3  23052901757       IBAGUÉ   \n",
      "4  23052501495  BUCARAMANGA   \n",
      "\n",
      "                                   entidad_remitente  \\\n",
      "0                          MUNICIPIO DE HONDA TOLIMA   \n",
      "1                                               DIAN   \n",
      "2  OFICINA DE APOYO PARA LOS JUZGADOS CIVILES MUN...   \n",
      "3  JUZGADO PRIMERO DE PEQUEÑAS CAUSAS Y COMPETENC...   \n",
      "4      JUZGADO QUINCE CIVIL MUNICIPAL DE BUCARAMANGA   \n",
      "\n",
      "                                             correo  \\\n",
      "0  notificacionjudicialhacienda@honda-tolima.gov.co   \n",
      "1               embarprodpa@scotiabankcolpatria.com   \n",
      "2                                                     \n",
      "3            j01pqccmiba@cendoj.ramajudicial.gov.co   \n",
      "4               j15cmbuc@cendoj.ramajudicial.gov.co   \n",
      "\n",
      "                                           direccion  \\\n",
      "0                                Carrera 12 N 12 17    \n",
      "1                                 CARRERA 7 # 24 89    \n",
      "2                                  Calle 15 # 10 61    \n",
      "3  Carrera 5 # 41 16 edificio F25 Bussines Center...   \n",
      "4                  PALACIO DE JUSTICIA- SEGUNDO PISO   \n",
      "\n",
      "                                 funcionario fecha_banco fecha_oficio  \\\n",
      "0                      GLORIA PATRICIA DIAGO  2023-05-26   2023-05-15   \n",
      "1                    DAZA GOMEZ NUBIA STELLA  2023-05-31   2023-05-26   \n",
      "2                 Diana Paola Cárdenas López  2023-05-23   2023-03-15   \n",
      "3  MARIA DEL PILAR JARAMILLO RICO Secretaria  2023-05-30   2023-05-19   \n",
      "4                  SANTIAGO HINESTROZA LAMUS  2023-05-25   2023-02-03   \n",
      "\n",
      "                                   referencia         cuenta  ...  \\\n",
      "0                                   NO TIENE   733499195010   ...   \n",
      "1                               No encontrado  680019193001   ...   \n",
      "2    11001 - 40 - 03 - 066 - 2019 - 00875 00                  ...   \n",
      "3  73001 - 41 - 89 - 001 - 2021 - 00135 - 00    730012051101  ...   \n",
      "4           680014003015 - 2022 - 00615 - 00   680012041802   ...   \n",
      "\n",
      "                         nombres      expediente      mes entidad_bancaria  \\\n",
      "0        JUAN JOSE ROJAS SANCHEZ         2019391  2023-05        COLPATRIA   \n",
      "1    JAIMES GUERRERO JUAN CARLOS  20230225001828  2023-05        COLPATRIA   \n",
      "2  EIDER JAVIER FONTECHA CAMACHO            NULL  2023-05        SANTANDER   \n",
      "3      Hugo Ramiro Rodríguez Roa            NULL  2023-05        COLPATRIA   \n",
      "4        ANDELFO RAMIREZ HINCAPI            NULL  2023-05        COLPATRIA   \n",
      "\n",
      "  estado_embargo tipo_documento tipo_embargo estado_demandado es_cliente  \\\n",
      "0     CONFIRMADO        EMBARGO     COACTIVO     SIN_PROCESAR          0   \n",
      "1     CONFIRMADO        EMBARGO     COACTIVO     SIN_PROCESAR          0   \n",
      "2      PROCESADO     DESEMBARGO     JUDICIAL        PROCESADO          0   \n",
      "3     CONFIRMADO        EMBARGO     JUDICIAL     SIN_PROCESAR          0   \n",
      "4     CONFIRMADO  REQUERIMIENTO     JUDICIAL     SIN_PROCESAR          0   \n",
      "\n",
      "                tipo_carta  \n",
      "0                           \n",
      "1                           \n",
      "2  NO_ES_CLIENTE_SIN_CARTA  \n",
      "3                           \n",
      "4                           \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Filas corregidas: 728\n",
      "Filas omitidas definitivamente: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Orden estándar de columnas\n",
    "expected_columns = [\n",
    "    \"id\", \"ciudad\", \"entidad_remitente\", \"correo\", \"direccion\", \"funcionario\", \"fecha_banco\",\n",
    "    \"fecha_oficio\", \"referencia\", \"cuenta\", \"identificacion\", \"tipo_identificacion_tipo\", \"montoaembargar\",\n",
    "    \"nombres\", \"expediente\", \"mes\", \"entidad_bancaria\", \"estado_embargo\", \"tipo_documento\",\n",
    "    \"tipo_embargo\", \"estado_demandado\", \"es_cliente\", \"tipo_carta\"\n",
    "]\n",
    "num_expected = len(expected_columns)\n",
    "\n",
    "csv_files = glob.glob(\"consulta detalle embargos-*.csv\")\n",
    "dataframes = []\n",
    "log_corregidas, log_omitidas = [], []\n",
    "\n",
    "for input_file in csv_files:\n",
    "    print(f\"Leyendo archivo: {input_file}\")\n",
    "    rows = []\n",
    "    with open(input_file, encoding=\"utf-8\") as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        headers = next(reader)\n",
    "        for idx, row in enumerate(reader, start=2):  # Empieza en 2 por el header\n",
    "            # Reparar filas con menos columnas (rellena con '')\n",
    "            if len(row) < num_expected:\n",
    "                row = row + [''] * (num_expected - len(row))\n",
    "                log_corregidas.append((input_file, idx, 'faltantes', len(row)))\n",
    "            # Reparar filas con más columnas (por comas en dirección/nombres)\n",
    "            if len(row) > num_expected:\n",
    "                # Heurística: pega campos extra en \"direccion\" (col 4)\n",
    "                extra = len(row) - num_expected\n",
    "                direccion = ','.join(row[4:4+1+extra])\n",
    "                fixed = row[:4] + [direccion] + row[4+1+extra:]\n",
    "                if len(fixed) == num_expected:\n",
    "                    row = fixed\n",
    "                    log_corregidas.append((input_file, idx, 'excedente', len(row)))\n",
    "                else:\n",
    "                    log_omitidas.append((input_file, idx, len(row), row))\n",
    "                    continue  # Omitir fila irrecuperable\n",
    "            # Asegura que el orden y cantidad de columnas sea correcto\n",
    "            if len(row) == num_expected:\n",
    "                rows.append(row)\n",
    "            else:\n",
    "                log_omitidas.append((input_file, idx, len(row), row))\n",
    "    df_temp = pd.DataFrame(rows, columns=expected_columns)\n",
    "    dataframes.append(df_temp)\n",
    "\n",
    "# Unión de todo\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# --- Limpieza y normalización ---\n",
    "# Númericas\n",
    "df['montoaembargar'] = pd.to_numeric(df['montoaembargar'], errors='coerce').fillna(0)\n",
    "\n",
    "# es_cliente: binariza bien todas las variantes, evita problemas de codificación futura\n",
    "def clean_es_cliente(val):\n",
    "    v = str(val).strip().upper()\n",
    "    return 1 if v in {'1', 'SI_ES_CLIENTE', 'CLIENTE', 'SI', 'SÍ', 'TRUE', 'Y', 'YES'} else 0\n",
    "\n",
    "df['es_cliente'] = df['es_cliente'].apply(clean_es_cliente).astype(int)\n",
    "\n",
    "# Categóricas: upper, strip y sin nulos (evita valores 'nan' de string)\n",
    "cat_cols = ['entidad_bancaria', 'ciudad', 'entidad_remitente', 'tipo_documento', 'tipo_embargo', 'estado_embargo', 'estado_demandado', 'tipo_carta', 'mes']\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper().replace({'NAN': '', 'NONE': '', 'NULL': ''})\n",
    "\n",
    "# Opcional: agrupa clases raras si hay muchas (puedes ajustar umbral)\n",
    "for col in ['tipo_embargo', 'estado_embargo']:\n",
    "    vc = df[col].value_counts()\n",
    "    raros = vc[vc < 10].index\n",
    "    df.loc[df[col].isin(raros), col] = 'OTRO'\n",
    "\n",
    "# Limpia fechas\n",
    "for col in ['fecha_banco', 'fecha_oficio']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Muestra por mes (ajusta a tu necesidad)\n",
    "frac_muestra = 0.07\n",
    "n_muestra = None\n",
    "df_muestreado = df.groupby('mes', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=frac_muestra, random_state=42) if n_muestra is None else x.sample(n=min(n_muestra, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Guarda resultado\n",
    "output_file = \"embargos_consolidado_mensual.csv\"\n",
    "df_muestreado.to_csv(output_file, index=False)\n",
    "print(f\"\\nArchivo final consolidado y muestreado: {output_file}\")\n",
    "print(f\"Filas originales: {len(df)}, tras muestreo: {len(df_muestreado)}\")\n",
    "print(\"\\nEjemplo de datos:\")\n",
    "print(df_muestreado.head())\n",
    "\n",
    "# Resumen de logs\n",
    "print(f\"\\nFilas corregidas: {len(log_corregidas)}\")\n",
    "print(f\"Filas omitidas definitivamente: {len(log_omitidas)}\")\n",
    "if log_omitidas:\n",
    "    print(\"Ejemplo de líneas omitidas:\", log_omitidas[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wir-zcpqFT49",
    "outputId": "cc9039f2-84b9-4a19-90f8-3fcc3580faa0"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3771256629.py, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 83\u001b[1;36m\u001b[0m\n\u001b[1;33m    objective='count:poisson', random_state=42\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------- CARGA Y LIMPIEZA DE DATOS -----------\n",
    "df = pd.read_csv('embargos_consolidado_mensual.csv')\n",
    "\n",
    "def agrupar_otros(df, col, min_freq=10):\n",
    "    freq = df[col].value_counts()\n",
    "    otros = freq[freq < min_freq].index\n",
    "    df[col] = df[col].apply(lambda x: 'OTRO' if x in otros else x)\n",
    "    return df\n",
    "\n",
    "for col in ['ciudad', 'entidad_remitente', 'tipo_embargo', 'estado_embargo']:\n",
    "    df[col] = df[col].fillna('OTRO').astype(str).str.strip().str.upper()\n",
    "    df = agrupar_otros(df, col, min_freq=10)\n",
    "\n",
    "df['montoaembargar'] = pd.to_numeric(df['montoaembargar'], errors='coerce')\n",
    "\n",
    "def clean_cliente(val):\n",
    "    v = str(val).strip().upper()\n",
    "    return 1 if v in {'1', 'SI', 'SI_ES_CLIENTE', 'CLIENTE', 'TRUE', 'SÍ', 'YES'} else 0\n",
    "df['es_cliente_bin'] = df['es_cliente'].apply(clean_cliente).astype(int)\n",
    "\n",
    "df['fecha_banco'] = pd.to_datetime(df['fecha_banco'], errors='coerce')\n",
    "df['año'] = pd.to_numeric(df['fecha_banco'].dt.year, errors='coerce')\n",
    "df['mes_num'] = pd.to_numeric(df['fecha_banco'].dt.month, errors='coerce')\n",
    "df = df.dropna(subset=['año', 'mes_num']).copy()\n",
    "df['año'] = df['año'].astype(int)\n",
    "df['mes_num'] = df['mes_num'].astype(int)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_ciudad = LabelEncoder(); le_entidad = LabelEncoder()\n",
    "le_tipo_embargo = LabelEncoder(); le_estado_embargo = LabelEncoder()\n",
    "df['ciudad_enc'] = le_ciudad.fit_transform(df['ciudad'])\n",
    "df['entidad_remitente_enc'] = le_entidad.fit_transform(df['entidad_remitente'])\n",
    "df['tipo_embargo_enc'] = le_tipo_embargo.fit_transform(df['tipo_embargo'])\n",
    "df['estado_embargo_enc'] = le_estado_embargo.fit_transform(df['estado_embargo'])\n",
    "\n",
    "df['mes_sin'] = np.sin(2 * np.pi * df['mes_num'] / 12.0)\n",
    "df['mes_cos'] = np.cos(2 * np.pi * df['mes_num'] / 12.0)\n",
    "df['mes_index'] = df['año'] * 12 + df['mes_num']\n",
    "\n",
    "# ------- Lags y media móvil para regresión por mes -------\n",
    "oficios_por_mes = df.groupby(['año','mes_num']).agg({\n",
    "    'id': 'count',\n",
    "    'identificacion': pd.Series.nunique,\n",
    "    'montoaembargar': 'sum'\n",
    "}).reset_index().sort_values(['año','mes_num'])\n",
    "\n",
    "oficios_por_mes['oficios_lag1'] = oficios_por_mes['id'].shift(1)\n",
    "oficios_por_mes['oficios_lag2'] = oficios_por_mes['id'].shift(2)\n",
    "oficios_por_mes['oficios_lag3'] = oficios_por_mes['id'].shift(3)\n",
    "oficios_por_mes['oficios_ma3'] = oficios_por_mes['id'].rolling(window=3).mean().shift(1)\n",
    "oficios_por_mes['demandados_lag1'] = oficios_por_mes['identificacion'].shift(1)\n",
    "oficios_por_mes['demandados_lag2'] = oficios_por_mes['identificacion'].shift(2)\n",
    "oficios_por_mes['demandados_ma3'] = oficios_por_mes['identificacion'].rolling(window=3).mean().shift(1)\n",
    "oficios_por_mes['mes_sin'] = np.sin(2 * np.pi * oficios_por_mes['mes_num'] / 12.0)\n",
    "oficios_por_mes['mes_cos'] = np.cos(2 * np.pi * oficios_por_mes['mes_num'] / 12.0)\n",
    "\n",
    "# ----------- VALIDACIÓN TEMPORAL -----------\n",
    "ultimo_año = oficios_por_mes['año'].max()\n",
    "train = oficios_por_mes[oficios_por_mes['año'] < ultimo_año]\n",
    "test  = oficios_por_mes[oficios_por_mes['año'] == ultimo_año]\n",
    "\n",
    "# ----------- REGRESIÓN: Total de oficios por mes -----------\n",
    "features_reg = ['año','mes_num','mes_sin','mes_cos','oficios_lag1','oficios_lag2','oficios_lag3','oficios_ma3']\n",
    "X_train = train[features_reg]\n",
    "y_train = train['id']\n",
    "X_test  = test[features_reg]\n",
    "y_test  = test['id']\n",
    "\n",
    "mask_train = ~(X_train.isnull().any(axis=1) | y_train.isnull())\n",
    "mask_test = ~(X_test.isnull().any(axis=1) | y_test.isnull())\n",
    "X_train_clean, y_train_clean = X_train[mask_train], y_train[mask_train]\n",
    "X_test_clean,  y_test_clean  = X_test[mask_test],  y_test[mask_test]\n",
    "\n",
    "regressor = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=7,\n",
    "    objective='count:poisson', random_state=42\n",
    "    base_score=np.mean(y_train_clean)\n",
    ")\n",
    "regressor.fit(X_train_clean, y_train_clean)\n",
    "y_pred = regressor.predict(X_test_clean)\n",
    "\n",
    "print(\"=== REGRESIÓN: Oficios por mes ===\")\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_clean, y_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(y_test_clean, y_pred))\n",
    "print(\"Promedio real:\", y_test_clean.mean())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(y_test_clean)), y_test_clean.values, 'o-', label='Real')\n",
    "plt.plot(range(len(y_pred)), y_pred, 'x--', label='Predicción')\n",
    "plt.title(\"Oficios por mes - Predicción vs Real\")\n",
    "plt.xlabel(\"Mes (test set)\")\n",
    "plt.ylabel(\"Cantidad de oficios\")\n",
    "plt.legend(); plt.grid(True, alpha=0.4); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------- REGRESIÓN: Demandados únicos por mes -----------\n",
    "features_dem = ['año','mes_num','mes_sin','mes_cos','demandados_lag1','demandados_lag2','demandados_ma3']\n",
    "X_train_d = train[features_dem]\n",
    "y_train_d = train['identificacion']\n",
    "X_test_d  = test[features_dem]\n",
    "y_test_d  = test['identificacion']\n",
    "\n",
    "mask_train_d = ~(X_train_d.isnull().any(axis=1) | y_train_d.isnull())\n",
    "mask_test_d = ~(X_test_d.isnull().any(axis=1) | y_test_d.isnull())\n",
    "X_train_d_clean, y_train_d_clean = X_train_d[mask_train_d], y_train_d[mask_train_d]\n",
    "X_test_d_clean,  y_test_d_clean  = X_test_d[mask_test_d],  y_test_d[mask_test_d]\n",
    "\n",
    "regressor_dem = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=7,\n",
    "    objective='count:poisson', random_state=42\n",
    "    base_score=np.mean(y_train_d_clean)\n",
    ")\n",
    "regressor_dem.fit(X_train_d_clean, y_train_d_clean)\n",
    "y_pred_d = regressor_dem.predict(X_test_d_clean)\n",
    "\n",
    "print(\"\\n=== REGRESIÓN: Demandados únicos por mes ===\")\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_d_clean, y_pred_d)))\n",
    "print(\"MAE:\", mean_absolute_error(y_test_d_clean, y_pred_d))\n",
    "print(\"Promedio real:\", y_test_d_clean.mean())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(y_test_d_clean)), y_test_d_clean.values, 'o-', label='Real')\n",
    "plt.plot(range(len(y_pred_d)), y_pred_d, 'x--', label='Predicción')\n",
    "plt.title(\"Demandados únicos por mes - Predicción vs Real\")\n",
    "plt.xlabel(\"Mes (test set)\")\n",
    "plt.ylabel(\"Cantidad de demandados únicos\")\n",
    "plt.legend(); plt.grid(True, alpha=0.4); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------- CLASIFICACIONES SOBRE DATAFRAME ORIGINAL -----------\n",
    "\n",
    "# Variables para clasificación\n",
    "features_clf = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'estado_embargo_enc', 'es_cliente_bin'\n",
    "]\n",
    "\n",
    "# --- 1. Coactivo o Judicial (tipo_embargo_enc) ---\n",
    "X = df[features_clf]\n",
    "y = df['tipo_embargo_enc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "clf = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                    subsample=0.9, colsample_bytree=0.8,\n",
    "                    eval_metric='mlogloss', use_label_encoder=False,\n",
    "                    tree_method=\"hist\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "le_tipo_embargo = LabelEncoder().fit(df['tipo_embargo'])\n",
    "labels_report = np.unique(np.concatenate([y_test, y_pred]))\n",
    "print(\"\\n=== CLASIFICACIÓN: Coactivo o Judicial ===\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    labels=labels_report,\n",
    "    target_names=le_tipo_embargo.inverse_transform(labels_report),\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# --- 2. Estado de oficio (embargo, desembargo, requerimientos) ---\n",
    "features_clf2 = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'tipo_embargo_enc', 'es_cliente_bin'\n",
    "]\n",
    "y2 = df['estado_embargo_enc']\n",
    "class_counts = y2.value_counts()\n",
    "clases_validas = class_counts[class_counts >= 2].index\n",
    "mask_validas = y2.isin(clases_validas)\n",
    "X2_valid = df[features_clf2][mask_validas]\n",
    "y2_valid = y2[mask_validas]\n",
    "\n",
    "le_estado_embargo = LabelEncoder()\n",
    "le_estado_embargo.fit(df['estado_embargo'][df['estado_embargo_enc'].isin(clases_validas)])\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2_valid, y2_valid, stratify=y2_valid, test_size=0.2, random_state=42)\n",
    "clf2 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf2.fit(X_train2, y_train2)\n",
    "y_pred2 = clf2.predict(X_test2)\n",
    "labels_report2 = np.unique(np.concatenate([y_test2, y_pred2]))\n",
    "print(\"\\n=== CLASIFICACIÓN: Estado de oficio ===\")\n",
    "print(classification_report(\n",
    "    y_test2, y_pred2,\n",
    "    labels=labels_report2,\n",
    "    target_names=le_estado_embargo.inverse_transform(labels_report2),\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# --- 3. Cliente o no cliente ---\n",
    "features_clf3 = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'tipo_embargo_enc', 'estado_embargo_enc'\n",
    "]\n",
    "y3 = df['es_cliente_bin']\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(df[features_clf3], y3, stratify=y3, test_size=0.2, random_state=42)\n",
    "scale_pos_weight = (y_train3 == 0).sum() / (y_train3 == 1).sum()\n",
    "clf3 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='auc', use_label_encoder=False,\n",
    "                     tree_method=\"hist\", scale_pos_weight=scale_pos_weight)\n",
    "clf3.fit(X_train3, y_train3)\n",
    "y_pred3 = clf3.predict(X_test3)\n",
    "labels_report3 = np.unique(np.concatenate([y_test3, y_pred3]))\n",
    "print(\"\\n=== CLASIFICACIÓN: Cliente o no cliente ===\")\n",
    "# Aquí son solo dos clases, puedes poner nombres fijos\n",
    "print(classification_report(\n",
    "    y_test3, y_pred3,\n",
    "    labels=labels_report3,\n",
    "    target_names=[\"NO_CLIENTE\", \"CLIENTE\"],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# --- 4. Ciudad ---\n",
    "features_for_ciudad = [c for c in features_clf + ['tipo_embargo_enc'] if c != 'ciudad_enc']\n",
    "y4 = df['ciudad_enc']\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(df[features_for_ciudad], y4, stratify=y4, test_size=0.2, random_state=42)\n",
    "clf4 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf4.fit(X_train4, y_train4)\n",
    "y_pred4 = clf4.predict(X_test4)\n",
    "labels_report4 = np.unique(np.concatenate([y_test4, y_pred4]))\n",
    "print(\"\\n=== CLASIFICACIÓN: Ciudad ===\")\n",
    "print(classification_report(\n",
    "    y_test4, y_pred4,\n",
    "    labels=labels_report4,\n",
    "    target_names=le_ciudad.inverse_transform(labels_report4),\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# --- 5. Entidad remitente ---\n",
    "features_for_entidad = [c for c in features_clf + ['tipo_embargo_enc'] if c != 'entidad_remitente_enc']\n",
    "y5 = df['entidad_remitente_enc']\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(df[features_for_entidad], y5, stratify=y5, test_size=0.2, random_state=42)\n",
    "clf5 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf5.fit(X_train5, y_train5)\n",
    "y_pred5 = clf5.predict(X_test5)\n",
    "labels_report5 = np.unique(np.concatenate([y_test5, y_pred5]))\n",
    "print(\"\\n=== CLASIFICACIÓN: Entidad remitente ===\")\n",
    "print(classification_report(\n",
    "    y_test5, y_pred5,\n",
    "    labels=labels_report5,\n",
    "    target_names=le_entidad.inverse_transform(labels_report5),\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------- CARGA Y LIMPIEZA DE DATOS -----------\n",
    "df = pd.read_csv('embargos_consolidado_mensual.csv')\n",
    "\n",
    "def agrupar_otros(df, col, min_freq=10):\n",
    "    freq = df[col].value_counts()\n",
    "    otros = freq[freq < min_freq].index\n",
    "    df[col] = df[col].apply(lambda x: 'OTRO' if x in otros else x)\n",
    "    return df\n",
    "\n",
    "for col in ['ciudad', 'entidad_remitente', 'tipo_embargo', 'estado_embargo']:\n",
    "    df[col] = df[col].fillna('OTRO').astype(str).str.strip().str.upper()\n",
    "    df = agrupar_otros(df, col, min_freq=10)\n",
    "\n",
    "df['montoaembargar'] = pd.to_numeric(df['montoaembargar'], errors='coerce')\n",
    "def clean_cliente(val):\n",
    "    v = str(val).strip().upper()\n",
    "    return 1 if v in {'1', 'SI', 'SI_ES_CLIENTE', 'CLIENTE', 'TRUE', 'SÍ', 'YES'} else 0\n",
    "df['es_cliente_bin'] = df['es_cliente'].apply(clean_cliente).astype(int)\n",
    "\n",
    "df['fecha_banco'] = pd.to_datetime(df['fecha_banco'], errors='coerce')\n",
    "df['año'] = pd.to_numeric(df['fecha_banco'].dt.year, errors='coerce')\n",
    "df['mes_num'] = pd.to_numeric(df['fecha_banco'].dt.month, errors='coerce')\n",
    "df = df.dropna(subset=['año', 'mes_num']).copy()\n",
    "df['año'] = df['año'].astype(int)\n",
    "df['mes_num'] = df['mes_num'].astype(int)\n",
    "\n",
    "le_ciudad = LabelEncoder(); le_entidad = LabelEncoder()\n",
    "le_tipo_embargo = LabelEncoder(); le_estado_embargo = LabelEncoder()\n",
    "df['ciudad_enc'] = le_ciudad.fit_transform(df['ciudad'])\n",
    "df['entidad_remitente_enc'] = le_entidad.fit_transform(df['entidad_remitente'])\n",
    "df['tipo_embargo_enc'] = le_tipo_embargo.fit_transform(df['tipo_embargo'])\n",
    "df['estado_embargo_enc'] = le_estado_embargo.fit_transform(df['estado_embargo'])\n",
    "\n",
    "df['mes_sin'] = np.sin(2 * np.pi * df['mes_num'] / 12.0)\n",
    "df['mes_cos'] = np.cos(2 * np.pi * df['mes_num'] / 12.0)\n",
    "df['mes_index'] = df['año'] * 12 + df['mes_num']\n",
    "\n",
    "# ------- Lags y media móvil para regresión por mes -------\n",
    "oficios_por_mes = df.groupby(['año','mes_num']).agg({\n",
    "    'id': 'count',\n",
    "    'identificacion': pd.Series.nunique,\n",
    "    'montoaembargar': 'sum'\n",
    "}).reset_index().sort_values(['año','mes_num'])\n",
    "\n",
    "oficios_por_mes['oficios_lag1'] = oficios_por_mes['id'].shift(1)\n",
    "oficios_por_mes['oficios_lag2'] = oficios_por_mes['id'].shift(2)\n",
    "oficios_por_mes['oficios_lag3'] = oficios_por_mes['id'].shift(3)\n",
    "oficios_por_mes['oficios_ma3'] = oficios_por_mes['id'].rolling(window=3).mean().shift(1)\n",
    "oficios_por_mes['demandados_lag1'] = oficios_por_mes['identificacion'].shift(1)\n",
    "oficios_por_mes['demandados_lag2'] = oficios_por_mes['identificacion'].shift(2)\n",
    "oficios_por_mes['demandados_ma3'] = oficios_por_mes['identificacion'].rolling(window=3).mean().shift(1)\n",
    "oficios_por_mes['mes_sin'] = np.sin(2 * np.pi * oficios_por_mes['mes_num'] / 12.0)\n",
    "oficios_por_mes['mes_cos'] = np.cos(2 * np.pi * oficios_por_mes['mes_num'] / 12.0)\n",
    "\n",
    "# ----------- VALIDACIÓN TEMPORAL -----------\n",
    "ultimo_año = oficios_por_mes['año'].max()\n",
    "train = oficios_por_mes[oficios_por_mes['año'] < ultimo_año]\n",
    "test  = oficios_por_mes[oficios_por_mes['año'] == ultimo_año]\n",
    "\n",
    "# ----------- REGRESIÓN: Total de oficios por mes -----------\n",
    "features_reg = ['año','mes_num','mes_sin','mes_cos','oficios_lag1','oficios_lag2','oficios_lag3','oficios_ma3']\n",
    "X_train = train[features_reg]\n",
    "y_train = train['id']\n",
    "X_test  = test[features_reg]\n",
    "y_test  = test['id']\n",
    "\n",
    "mask_train = ~(X_train.isnull().any(axis=1) | y_train.isnull())\n",
    "mask_test = ~(X_test.isnull().any(axis=1) | y_test.isnull())\n",
    "X_train_clean, y_train_clean = X_train[mask_train], y_train[mask_train]\n",
    "X_test_clean,  y_test_clean  = X_test[mask_test],  y_test[mask_test]\n",
    "\n",
    "regressor = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=7,\n",
    "    objective='count:poisson', random_state=42\n",
    "    base_score=np.mean(y_train_d_clean)\n",
    ")\n",
    "regressor.fit(X_train_clean, y_train_clean)\n",
    "y_pred = regressor.predict(X_test_clean)\n",
    "\n",
    "# ----------- REGRESIÓN: Demandados únicos por mes -----------\n",
    "features_dem = ['año','mes_num','mes_sin','mes_cos','demandados_lag1','demandados_lag2','demandados_ma3']\n",
    "X_train_d = train[features_dem]\n",
    "y_train_d = train['identificacion']\n",
    "X_test_d  = test[features_dem]\n",
    "y_test_d  = test['identificacion']\n",
    "\n",
    "mask_train_d = ~(X_train_d.isnull().any(axis=1) | y_train_d.isnull())\n",
    "mask_test_d = ~(X_test_d.isnull().any(axis=1) | y_test_d.isnull())\n",
    "X_train_d_clean, y_train_d_clean = X_train_d[mask_train_d], y_train_d[mask_train_d]\n",
    "X_test_d_clean,  y_test_d_clean  = X_test_d[mask_test_d],  y_test_d[mask_test_d]\n",
    "\n",
    "regressor_dem = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=7,\n",
    "    objective='count:poisson', random_state=42\n",
    "    base_score=np.mean(y_train_d_clean)\n",
    ")\n",
    "regressor_dem.fit(X_train_d_clean, y_train_d_clean)\n",
    "y_pred_d = regressor_dem.predict(X_test_d_clean)\n",
    "\n",
    "# ============ GUARDAR CSVs DE REGRESIÓN ============\n",
    "# Oficios por mes\n",
    "df_pred_oficios = test.iloc[mask_test.values][['año','mes_num']].copy()\n",
    "df_pred_oficios['real_oficios'] = y_test_clean.values\n",
    "df_pred_oficios['pred_oficios'] = y_pred\n",
    "df_pred_oficios['mes'] = df_pred_oficios['año'].astype(str) + \"-\" + df_pred_oficios['mes_num'].astype(str).str.zfill(2)\n",
    "df_pred_oficios[['mes', 'real_oficios', 'pred_oficios']].to_csv(\"predicciones_oficios_por_mes.csv\", index=False)\n",
    "print(\"CSV generado: predicciones_oficios_por_mes.csv\")\n",
    "\n",
    "# Demandados por mes\n",
    "df_pred_demandados = test.iloc[mask_test_d.values][['año','mes_num']].copy()\n",
    "df_pred_demandados['real_demandados'] = y_test_d_clean.values\n",
    "df_pred_demandados['pred_demandados'] = y_pred_d\n",
    "df_pred_demandados['mes'] = df_pred_demandados['año'].astype(str) + \"-\" + df_pred_demandados['mes_num'].astype(str).str.zfill(2)\n",
    "df_pred_demandados[['mes', 'real_demandados', 'pred_demandados']].to_csv(\"predicciones_demandados_por_mes.csv\", index=False)\n",
    "print(\"CSV generado: predicciones_demandados_por_mes.csv\")\n",
    "\n",
    "# ============ CLASIFICACIONES Y MÉTRICAS ============\n",
    "features_clf = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'estado_embargo_enc', 'es_cliente_bin'\n",
    "]\n",
    "# 1. Tipo Embargo (coactivo/judicial)\n",
    "X = df[features_clf]\n",
    "y = df['tipo_embargo_enc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "clf = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                    subsample=0.9, colsample_bytree=0.8,\n",
    "                    eval_metric='mlogloss', use_label_encoder=False,\n",
    "                    tree_method=\"hist\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "le_tipo_embargo = LabelEncoder().fit(df['tipo_embargo'])\n",
    "labels_report = np.unique(np.concatenate([y_test, y_pred]))\n",
    "\n",
    "# 2. Estado Oficio\n",
    "features_clf2 = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'tipo_embargo_enc', 'es_cliente_bin'\n",
    "]\n",
    "y2 = df['estado_embargo_enc']\n",
    "class_counts = y2.value_counts()\n",
    "clases_validas = class_counts[class_counts >= 2].index\n",
    "mask_validas = y2.isin(clases_validas)\n",
    "X2_valid = df[features_clf2][mask_validas]\n",
    "y2_valid = y2[mask_validas]\n",
    "le_estado_embargo = LabelEncoder()\n",
    "le_estado_embargo.fit(df['estado_embargo'][df['estado_embargo_enc'].isin(clases_validas)])\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2_valid, y2_valid, stratify=y2_valid, test_size=0.2, random_state=42)\n",
    "clf2 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf2.fit(X_train2, y_train2)\n",
    "y_pred2 = clf2.predict(X_test2)\n",
    "labels_report2 = np.unique(np.concatenate([y_test2, y_pred2]))\n",
    "\n",
    "# 3. Cliente/no cliente\n",
    "features_clf3 = [\n",
    "    'entidad_remitente_enc', 'mes_num', 'montoaembargar',\n",
    "    'tipo_embargo_enc', 'estado_embargo_enc'\n",
    "]\n",
    "y3 = df['es_cliente_bin']\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(df[features_clf3], y3, stratify=y3, test_size=0.2, random_state=42)\n",
    "scale_pos_weight = (y_train3 == 0).sum() / (y_train3 == 1).sum()\n",
    "clf3 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='auc', use_label_encoder=False,\n",
    "                     tree_method=\"hist\", scale_pos_weight=scale_pos_weight)\n",
    "clf3.fit(X_train3, y_train3)\n",
    "y_pred3 = clf3.predict(X_test3)\n",
    "labels_report3 = np.unique(np.concatenate([y_test3, y_pred3]))\n",
    "\n",
    "# 4. Ciudad\n",
    "features_for_ciudad = [c for c in features_clf + ['tipo_embargo_enc'] if c != 'ciudad_enc']\n",
    "y4 = df['ciudad_enc']\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(df[features_for_ciudad], y4, stratify=y4, test_size=0.2, random_state=42)\n",
    "clf4 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf4.fit(X_train4, y_train4)\n",
    "y_pred4 = clf4.predict(X_test4)\n",
    "labels_report4 = np.unique(np.concatenate([y_test4, y_pred4]))\n",
    "\n",
    "# 5. Entidad Remitente\n",
    "features_for_entidad = [c for c in features_clf + ['tipo_embargo_enc'] if c != 'entidad_remitente_enc']\n",
    "y5 = df['entidad_remitente_enc']\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(df[features_for_entidad], y5, stratify=y5, test_size=0.2, random_state=42)\n",
    "clf5 = XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "                     subsample=0.9, colsample_bytree=0.8,\n",
    "                     eval_metric='mlogloss', use_label_encoder=False,\n",
    "                     tree_method=\"hist\")\n",
    "clf5.fit(X_train5, y_train5)\n",
    "y_pred5 = clf5.predict(X_test5)\n",
    "labels_report5 = np.unique(np.concatenate([y_test5, y_pred5]))\n",
    "\n",
    "# ============ GUARDAR METRICAS DE CLASIFICACIÓN =============\n",
    "def report_to_df(report, modelo, target_names):\n",
    "    df_metrics = pd.DataFrame(report).transpose().reset_index()\n",
    "    df_metrics = df_metrics.rename(columns={'index': 'clase'})\n",
    "    df_metrics['modelo'] = modelo\n",
    "    df_metrics = df_metrics[df_metrics['clase'].isin(target_names)]\n",
    "    df_metrics = df_metrics.rename(columns={\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1-score': 'f1',\n",
    "        'support': 'soporte'\n",
    "    })\n",
    "    df_metrics = df_metrics[['modelo', 'clase', 'precision', 'recall', 'f1', 'soporte']]\n",
    "    return df_metrics\n",
    "\n",
    "dfs = []\n",
    "\n",
    "report_te = classification_report(\n",
    "    y_test, y_pred, output_dict=True,\n",
    "    target_names=le_tipo_embargo.inverse_transform(labels_report), zero_division=0\n",
    ")\n",
    "dfs.append(report_to_df(report_te, \"Tipo Embargo\", le_tipo_embargo.inverse_transform(labels_report)))\n",
    "\n",
    "report_ee = classification_report(\n",
    "    y_test2, y_pred2, output_dict=True,\n",
    "    target_names=le_estado_embargo.inverse_transform(labels_report2), zero_division=0\n",
    ")\n",
    "dfs.append(report_to_df(report_ee, \"Estado Oficio\", le_estado_embargo.inverse_transform(labels_report2)))\n",
    "\n",
    "report_cli = classification_report(\n",
    "    y_test3, y_pred3, output_dict=True,\n",
    "    target_names=[\"NO_CLIENTE\", \"CLIENTE\"], zero_division=0\n",
    ")\n",
    "dfs.append(report_to_df(report_cli, \"Es Cliente\", [\"NO_CLIENTE\", \"CLIENTE\"]))\n",
    "\n",
    "report_ciud = classification_report(\n",
    "    y_test4, y_pred4, output_dict=True,\n",
    "    target_names=le_ciudad.inverse_transform(labels_report4), zero_division=0\n",
    ")\n",
    "dfs.append(report_to_df(report_ciud, \"Ciudad\", le_ciudad.inverse_transform(labels_report4)))\n",
    "\n",
    "report_ent = classification_report(\n",
    "    y_test5, y_pred5, output_dict=True,\n",
    "    target_names=le_entidad.inverse_transform(labels_report5), zero_division=0\n",
    ")\n",
    "dfs.append(report_to_df(report_ent, \"Entidad Remitente\", le_entidad.inverse_transform(labels_report5)))\n",
    "\n",
    "df_metrics_all = pd.concat(dfs, ignore_index=True)\n",
    "df_metrics_all.to_csv(\"resultados_clasificaciones.csv\", index=False)\n",
    "print(\"CSV generado: resultados_clasificaciones.csv\")\n",
    "\n",
    "print(\"\\n¡Todos los CSV están listos para usar en tu dashboard!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronosticos extendidos a 12 meses\n",
    "Los siguientes pasos generan las validaciones historicas y los pronosticos futuros (hasta 12 meses) para oficios y demandados, incorporando bandas de confianza y niveles de riesgo por horizonte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "HORIZON = 12\n",
    "Z_VALUE = 1.96\n",
    "CONSOLIDADO = \"embargos_consolidado_mensual.csv\"\n",
    "\n",
    "def confidence_label(h):\n",
    "    if h <= 3:\n",
    "        return \"Alta\"\n",
    "    if h <= 6:\n",
    "        return \"Media\"\n",
    "    return \"Baja\"\n",
    "\n",
    "def compute_interval(scale, h):\n",
    "    base = scale if scale > 0 else 1.0\n",
    "    return Z_VALUE * base * np.sqrt(max(1, h))\n",
    "\n",
    "def prepare_monthly_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['fecha_banco'] = pd.to_datetime(df['fecha_banco'], errors='coerce')\n",
    "    df = df.dropna(subset=['fecha_banco']).copy()\n",
    "    df['año'] = df['fecha_banco'].dt.year.astype(int)\n",
    "    df['mes_num'] = df['fecha_banco'].dt.month.astype(int)\n",
    "    monthly = df.groupby(['año', 'mes_num']).agg({\n",
    "        'id': 'count',\n",
    "        'identificacion': pd.Series.nunique,\n",
    "        'montoaembargar': 'sum'\n",
    "    }).reset_index().sort_values(['año', 'mes_num'])\n",
    "    monthly = monthly.rename(columns={\n",
    "        'id': 'oficios',\n",
    "        'identificacion': 'demandados',\n",
    "        'montoaembargar': 'monto_total'\n",
    "    })\n",
    "    monthly['fecha'] = pd.to_datetime({'year': monthly['año'], 'month': monthly['mes_num'], 'day': 1})\n",
    "    monthly = monthly.set_index('fecha').asfreq('MS', fill_value=0).reset_index()\n",
    "    monthly['año'] = monthly['fecha'].dt.year\n",
    "    monthly['mes_num'] = monthly['fecha'].dt.month\n",
    "    monthly['mes'] = monthly['año'].astype(str) + \"-\" + monthly['mes_num'].astype(str).str.zfill(2)\n",
    "    monthly['mes_sin'] = np.sin(2 * np.pi * monthly['mes_num'] / 12.0)\n",
    "    monthly['mes_cos'] = np.cos(2 * np.pi * monthly['mes_num'] / 12.0)\n",
    "    for lag in [1, 2, 3]:\n",
    "        monthly[f\"oficios_lag{lag}\"] = monthly['oficios'].shift(lag)\n",
    "        monthly[f\"demandados_lag{lag}\"] = monthly['demandados'].shift(lag)\n",
    "    monthly['oficios_ma3'] = monthly['oficios'].rolling(window=3).mean().shift(1)\n",
    "    monthly['demandados_ma3'] = monthly['demandados'].rolling(window=3).mean().shift(1)\n",
    "    return monthly\n",
    "\n",
    "def train_and_forecast(monthly, target_col, feature_cols, lag_cols, ma_col, stub):\n",
    "    usable = monthly.dropna(subset=feature_cols + [target_col]).copy()\n",
    "    if usable.empty:\n",
    "        print(f\"No hay datos suficientes para {stub}.\")\n",
    "        return None\n",
    "    last_year = usable['año'].max()\n",
    "    train = usable[usable['año'] < last_year]\n",
    "    test = usable[usable['año'] == last_year]\n",
    "    if train.empty or test.empty:\n",
    "        print(f\"Se requieren al menos dos años completos para {stub}.\")\n",
    "        return None\n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train[target_col]\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test[target_col]\n",
    "    mask_train = ~(X_train.isnull().any(axis=1) | y_train.isnull())\n",
    "    mask_test = ~(X_test.isnull().any(axis=1) | y_test.isnull())\n",
    "    X_train_clean, y_train_clean = X_train[mask_train], y_train[mask_train]\n",
    "    X_test_clean, y_test_clean = X_test[mask_test], y_test[mask_test]\n",
    "    if X_train_clean.empty or X_test_clean.empty:\n",
    "        print(f\"No hay filas limpias para {stub}.\")\n",
    "        return None\n",
    "    reg = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        objective=\"count:poisson\",\n",
    "        random_state=42,\n",
    "        base_score=float(y_train_clean.mean())\n",
    "    )\n",
    "    reg.fit(X_train_clean, y_train_clean)\n",
    "    y_pred = reg.predict(X_test_clean)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_test_clean, y_pred)))\n",
    "    mae = float(mean_absolute_error(y_test_clean, y_pred))\n",
    "    residuals = y_test_clean.values - y_pred\n",
    "    residual_scale = float(np.std(residuals)) if len(residuals) > 1 else mae\n",
    "    residual_scale = residual_scale if residual_scale > 0 else max(mae, 1.0)\n",
    "    validation_df = test.loc[mask_test, ['mes', target_col]].copy()\n",
    "    validation_df[f\"pred_{stub}\"] = y_pred\n",
    "    validation_df = validation_df.rename(columns={target_col: f\"real_{stub}\"})\n",
    "    validation_df = validation_df[['mes', f\"real_{stub}\", f\"pred_{stub}\"]]\n",
    "    validation_path = f\"predicciones_{stub}_validacion.csv\"\n",
    "    validation_df.to_csv(validation_path, index=False)\n",
    "    print(f\"[OK] Guardado {validation_path} | RMSE={rmse:.2f} MAE={mae:.2f}\")\n",
    "    mask_full = ~(usable[feature_cols].isnull().any(axis=1) | usable[target_col].isnull())\n",
    "    X_full = usable.loc[mask_full, feature_cols]\n",
    "    y_full = usable.loc[mask_full, target_col]\n",
    "    reg_full = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        objective=\"count:poisson\",\n",
    "        random_state=42,\n",
    "        base_score=float(y_full.mean())\n",
    "    )\n",
    "    reg_full.fit(X_full, y_full)\n",
    "    recent_vals = y_full.tail(3).tolist()\n",
    "    if not recent_vals:\n",
    "        recent_vals = [float(y_full.mean())] * 3\n",
    "    while len(recent_vals) < 3:\n",
    "        recent_vals.insert(0, float(np.mean(recent_vals)))\n",
    "    recent = recent_vals[-3:]\n",
    "    year = int(monthly['año'].iloc[-1])\n",
    "    month = int(monthly['mes_num'].iloc[-1])\n",
    "    future_rows = []\n",
    "    for horizon in range(1, HORIZON + 1):\n",
    "        month += 1\n",
    "        if month > 12:\n",
    "            month = 1\n",
    "            year += 1\n",
    "        lag1_val = recent[-1]\n",
    "        lag2_val = recent[-2]\n",
    "        lag3_val = recent[-3]\n",
    "        ma_val = np.mean(recent)\n",
    "        feature_row = {\n",
    "            'año': year,\n",
    "            'mes_num': month,\n",
    "            'mes_sin': np.sin(2 * np.pi * month / 12.0),\n",
    "            'mes_cos': np.cos(2 * np.pi * month / 12.0),\n",
    "            lag_cols[0]: lag1_val,\n",
    "            lag_cols[1]: lag2_val,\n",
    "            lag_cols[2]: lag3_val,\n",
    "            ma_col: ma_val\n",
    "        }\n",
    "        pred_value = float(reg_full.predict(pd.DataFrame([feature_row]))[0])\n",
    "        pred_value = max(0.0, pred_value)\n",
    "        intervalo = compute_interval(residual_scale, horizon)\n",
    "        future_rows.append({\n",
    "            'mes': f\"{year}-{str(month).zfill(2)}\",\n",
    "            f\"pred_{stub}\": round(pred_value, 2),\n",
    "            'limite_inferior': max(0.0, round(pred_value - intervalo, 2)),\n",
    "            'limite_superior': round(pred_value + intervalo, 2),\n",
    "            'nivel_confianza': confidence_label(horizon),\n",
    "            'horizonte_meses': horizon\n",
    "        })\n",
    "        recent = [recent[1], recent[2], pred_value]\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "    future_path = f\"predicciones_{stub}_futuro.csv\"\n",
    "    future_df.to_csv(future_path, index=False)\n",
    "    print(f\"[OK] Guardado {future_path} | Proximo mes {future_df.iloc[0]['mes']} -> {future_df.iloc[0][f'pred_{stub}']}\")\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'validation': validation_df,\n",
    "        'future': future_df\n",
    "    }\n",
    "\n",
    "monthly_ts = prepare_monthly_df(CONSOLIDADO)\n",
    "\n",
    "oficios_features = ['año', 'mes_num', 'mes_sin', 'mes_cos', 'oficios_lag1', 'oficios_lag2', 'oficios_lag3', 'oficios_ma3']\n",
    "demandados_features = ['año', 'mes_num', 'mes_sin', 'mes_cos', 'demandados_lag1', 'demandados_lag2', 'demandados_lag3', 'demandados_ma3']\n",
    "\n",
    "result_oficios = train_and_forecast(\n",
    "    monthly_ts,\n",
    "    target_col='oficios',\n",
    "    feature_cols=oficios_features,\n",
    "    lag_cols=['oficios_lag1', 'oficios_lag2', 'oficios_lag3'],\n",
    "    ma_col='oficios_ma3',\n",
    "    stub='oficios'\n",
    " )\n",
    "\n",
    "result_demandados = train_and_forecast(\n",
    "    monthly_ts,\n",
    "    target_col='demandados',\n",
    "    feature_cols=demandados_features,\n",
    "    lag_cols=['demandados_lag1', 'demandados_lag2', 'demandados_lag3'],\n",
    "    ma_col='demandados_ma3',\n",
    "    stub='demandados'\n",
    " )\n",
    "\n",
    "if result_oficios:\n",
    "    display(result_oficios['validation'].head())\n",
    "    display(result_oficios['future'].head())\n",
    "\n",
    "if result_demandados:\n",
    "    display(result_demandados['validation'].head())\n",
    "    display(result_demandados['future'].head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
